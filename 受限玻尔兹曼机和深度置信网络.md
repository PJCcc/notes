#### 受限玻尔兹曼机和深度置信网络
##### 引言
>  梯度弥散一直是困扰着深度神经网络的发展，那么如何解决梯度弥散问题呢？多伦多大学的Geoff Hinton提出了设想：**受限玻尔兹曼机**（Restricted Boltzmann Machines, RBM），即一类具有两层结构的、对称链接无自反馈的随机神经网络模型（一种特殊的马尔科夫随机场）。
>  **受限玻尔兹曼机**是一种可用**随机神经网络**（stochastic neural network）来解释的**概率图模型**（probabilistic graphical model）。RBM是Smolensky于1986年在**波尔兹曼机**（Boltzmann Machine，BM）基础上提出的，所谓“随机”是指网络中的神经元是随机神经元，输出状态只有两种（未激活和激活），状态的具体取值根据概率统计法则来决定。RBM理论是Hinton在2006年提出基于RBM的（Deep Belief Network）模型，大量学者开始研究RBM的理论及其应用。

##### 1. 玻尔兹曼机结构
玻尔兹曼机是二值的马尔科夫随机场(Markov Random Filed)，一个玻尔兹曼机可以表示为带权重的无向图：
![图1](https://upload.wikimedia.org/wikipedia/commons/7/7a/Boltzmannexamplev1.png)
玻尔兹曼机模型的结构为层间、层内全连接。
##### 2. 受限制玻尔兹曼机结构
区别于玻尔兹曼机，受限玻尔兹曼机可见单元和隐含单元之间存在连接，而隐单元两两之间和可见单元两两之间不存在连接，也就是层间全连接，层内无连接。

RBM可以表示成一个二分图模型，所有每一个可见层节点和隐藏层节点都有两种状态：处于激活状态时值为1，未被激活状态值为0。这里的0和1状态的意义是代表了模型会选取哪些节点来使用，处于激活状态的节点被使用，未处于激活状态的节点未被使用。节点的激活概率由可见层和隐藏层节点的分布函数计算。
![图2](https://img-blog.csdn.net/20130628222803078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbXl0ZXN0bXk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
如图1所示，一个RBM包含一个由随机的隐单元构成的隐藏层（一般是伯努利分布）和一个由随机的可见（观测）单元构成的可见（观测）层（一般是伯努利分布或高斯分布）。

##### 3. 受限玻尔兹曼机参数学习
一个RBM中，$v$表示所有可见单元，$h$表示所有隐单元。要想确定该模型，只要能够得到模型三个参数$\theta=\{W,A,B\}$即可。分别是权重矩阵$W$，可见层单元偏置$A$，隐藏层单元偏置$B$。
假设一个RBM有$n$个可见单元和$m$个隐单元，用$v_i$表示第$i$个可见单元，$ h_i $表示第$j$个隐单元，它的参数形式为：
$W=\{w_{i,j}\in R^{n\times m}\}$ 其中$W_{i,j}$表示第$i$个可见单元和第$j$个隐单元之间的权值
$A=\{a_i\in R^m\}$ 其中$a_i$表示第$i$个可见单元的偏置阈值
$B=\{b_j\in R^n\}$ 其中$b_j$表示第$j$个可见单元的偏置阈值

对于一组给定状态下的$(v, h)$值，假设可见层单元和隐藏层单元均服从伯努利分布，RBM的能量公式是：$$E(v,h|\theta)=-\sum_{i=1}^n a_iv_i-\sum_{j=1}^mb_jh_j-\sum_{i=1}^n\sum_{j=1}^mv_iW_{ij}h_j$$
其中，$\theta=\{W_{ij},a_i,b_j\}$是RBM模型的参数（均为实数），能量函数表示在每一个可见节点的取值和每一个隐藏层节点的取值之间都存在一个能量值（这里的能量仅仅是对应了物理学的概念，并没有额外的意义）。

对该能量函数指数化和正则化后可以得到可见层节点集合和隐藏层节点集合分别处于某一种状态下$(v,h)$联合概率分布公式：$$P(v,h|\theta)=\frac{e^{-E(v,h|\theta)}}{Z(\theta)}$$
$$Z(\theta)=\sum_{v,h}e^{-E(v,h|\theta)}$$
其中，$Z(\theta)$为归一化因子或配分函数（partition function），表示对可见层和隐藏层节点集合的所有可能状态的（能量指数）求和。

对于参数的求解往往采用似然函数求导的方法。已知联合概率分布$P(v,h|\theta)$，通过对隐藏层节点集合的所有状态求和，可以得到可见层节点集合的边缘分布$P(v|\theta)$：
$$P(v|\theta)=\frac{1}{Z(\theta)}\sum_he^{-E(v,h|\theta)}$$
边缘分布表示的是可见层节点集合处于某一种状态分布下的概率。

由于RBM模型的特殊的层间连接、层内无连接的结构，它具有以下重要性质：
1）在给定可见单元的状态时，各隐藏层单元的激活状态之间是条件独立的。此时，第$j$个隐单元的激活概率为：
$$P(h_j=1|v)=\sigma(b_j+\sum_iv_iW_{ij})$$
2）相应的，当给定隐单元的状态时，可见单元的激活概率同样是条件独立的：
$$P(v_i=1|h)=\sigma(a_i+\sum_jW_{ij}h_j)$$
其中，$\sigma(x)=\frac{1}{1+exp(-x)}$是sigmoid函数，其函数曲线如下图所示：
![](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike80%2C5%2C5%2C80%2C26/sign=892a5cb4c55c10383073c690d378f876/c9fcc3cec3fdfc03f23fbf16d73f8794a5c226dc.jpg)
采用该函数用作每一层节点的激活概率公式的原因是：sigmoid函数的定义域是 $(-\infty,+\infty)$，值域处于$(0,1)$之间。也就是说，无论模型的可见层输入节点数据处于一个多大的范围内，都可以通过sigmoid函数求得它相应的函数值，即节点的激活概率值。

##### 4. 模型参数求解
确定RBM模型需要求解模型的三个参数$\theta=\{W_{ij},a_i,b_j\}$，下面围绕参数的求解进行分析。

参数求解用到了似然函数的对数对参数求导。从$P(v|\theta)=\frac{1}{Z(\theta)}\sum_he^{-E({v,h|\theta})}$可知，能量$E$和概率$P$成反比，通过最大化$P$来最小化$E$。最大化似然函数常用方法是梯度上升法，梯度上升法是指对参数进行修改按照以下公式：
$$\theta=\theta+\mu\frac{\partial lnP(v)}{\partial\theta}$$
通过求$lnP(v)$关于$\theta$的导数，即$\Delta\theta$，然后对原$\theta$值进行修改。如此迭代使似然函数$P$最大，从而使能量$E$最小。

对数似然函数的格式：$lnP(v^t)$， $v^t$表示模型的输入数据。

然后对$\{W_{ij},a_i,b_j\}$里的参数分别进行求导，详细的推导过程就不写了：
$$\frac{\partial lnP(v^t)}{\partial w_{i,j}}=P(h_j=1|v^t)v_i^t-\sum_vP(v)P(h_j=1|v)v_i$$
$$\frac{\partial lnP(v^t)}{\partial a_i}=v_i^t-\sum_vP(v)v_i$$
$$\frac{\partial lnP(v^t)}{\partial b_j}=P(h_j=1|v^t)-\sum_vP(v)P(h_j=1|v)$$
由于上面三式的第二项中都含有$P(v)$，$P(v)$中仍然含有参数，所以它是式中求不出来的。所以，有很多人就提出了一些通过采样逼近的方法来求每一个式子中的第二项。

##### 5. 模型训练算法
##### 5.1 Gibbs采样算法
##### 5.2 CD-k算法
##### 6.模型的评估
##### 7深度置信网络
未完待续。。。
###### 参考文献   [1]https://www.cnblogs.com/jhding/p/5687696.html   [2]https://blog.csdn.net/u010223750/article/details/60882390   [3]Hinton G. A practical guide to training restricted Boltzmann machines[J]. Momentum, 2010, 9(1): 926.   [4]Bengio Y, Lamblin P, Popovici D, et al. Greedy layer-wise training of deep networks[J]. Advances in neural information processing systems, 2007, 19: 153.[5]Hinton G E, Osindero S, Teh Y W. A fast learning algorithm for deep belief nets[J]. Neural computation, 2006, 18(7): 1527-1554.   [5]